标题,文章简称,\cite{},发表年月,10大原则,4个阶段,一作,一作单位,通讯,通讯单位,具身任务,方法论,发表情况,链接,备注/讨论点,父记录
A Model-Agnostic Approach for Semantically Driven Disambiguation in Human-Robot Interaction,Fethiye Irmak et al.,dogan2025modelagnosticapproachsemanticallydriven,25.04,可信-准确,指令理解,Fethiye Irmak Dogan,KTH Royal Institute of Technology,,,Object Search,"LLM, 迭代式交互, 不确定度",IEEE International Conference on Robot & Human Interactive Communication (RO-MAN),https://arxiv.org/abs/2409.17004,关键词：指令歧义；任务：自然语言描述的定位，不是3D Grounding,
AGENTSAFE: Benchmarking the Safety of Embodied Agents on Hazardous Instructions,AGENTSAFE,liu2025agentsafebenchmarkingsafetyembodied,25.06,安全-抗攻击,指令理解,Aishan Liu,BUAA,,,"EQA(2D), VLA, VLN, Planning","Benchmark, MLLM, Attack Paradigm, AI2-THOR",,https://arxiv.org/abs/2506.14697,,
Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation,EAsafetyBench,wang2025advancingembodiedagentsecurity,25.04,安全-抗攻击,指令理解,Ning Wang,CQU,Chuan Ma,CQU,EQA(2D),Benchmark,,https://arxiv.org/abs/2504.15699,,
Adversarial Attacks on Robotic Vision Language Action Models,Eliot Krzysztof et al.,jones2025adversarialattacksroboticvision,25.06,安全-抗攻击,指令理解,Eliot Krzysztof Jones,Gray Swan AI,Eliot Krzysztof Jones1,Gray Swan AI,VLA,adversarial attack,,https://arxiv.org/abs/2506.03350,,
Adversarial Training for Multimodal Large Language Models against Jailbreak Attacks,ProEAT,lu2025adversarialtrainingmultimodallarge,25.03,安全-抗攻击,指令理解,Liming Lu,NJUST,,,EQA(2D),"MLLM, adversarial attack, 指令微调",,https://arxiv.org/abs/2503.04833,,
BadNAVer: Exploring Jailbreak Attacks On Vision-and-Language Navigation,BadNAVer,lyu2025badnaverexploringjailbreakattacks,25.05,安全-抗攻击,指令理解,Wenqi Lyu,AIML,Qi Wu,AIML,VLN,MLLM,,https://arxiv.org/abs/2505.12443,,
Concept Enhancement Engineering: A Lightweight and Efficient Robust Defense Against Jailbreak Attacks in Embodied AI,CEE,yang2025conceptenhancementengineeringlightweight,25.04,安全-抗攻击,指令理解,Jirui Yang; Zheyu Lin,FDU,,,"Planning, EQA(2D)",MLLM,,https://arxiv.org/abs/2504.13201,,
DoRO: Disambiguation of Referred Object for Embodied Agents,DoRO,pramanick2022doro,22.1,可信-准确,指令理解,Pradip Pramanick,TCS Research,Chayan Sarkar,TCS Research,Grounding (2D),小模型,"IEEE Robotics and Automation Letters, 2022",https://ieeexplore.ieee.org/abstract/document/9846930,关键词：模糊指令,
Embodied Instruction Following in Unknown Environments,Wu et al.,wu2025embodiedinstructionfollowingunknown,25.07,可信-可靠,指令理解,,,,,,,,,,
Embodied Multi-Agent Task Planning from Ambiguous Instruction,Liu et al.,liu2022embodied,22.06,可信-准确,指令理解,Xinzhu Liu,THU,Huaping Liu,THU,Planning,"Benchmark, 小模型",RSS 22,https://web.archive.org/web/20220704170254id_/http://www.roboticsproceedings.org/rss18/p032.pdf,关键词：模糊指令、多智能体,
Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning,Ramrakhya et al.,ramrakhya2025grounding,25.04,可信-准确,指令理解,Ram Ramrakhya,"Georgia Institute of Technology, Meta",,,EQA(2D),"Benchmark, MLLM, 强化学习",,https://arxiv.org/abs/2504.00907,关键词：模糊指令/指令歧义,
Improving Grounded Natural Language Understanding through Human-Robot Dialog,Thomason et al.,thomason2019improving,19.05,可信-准确,指令理解,Jesse Thomason,University of Washington,,,"Navigation, delivery, relocation",迭代式交互,ICRA 19,https://ieeexplore.ieee.org/abstract/document/8794287,关键词：指令对话,
Inner Monologue: Embodied Reasoning through Planning with Language Models,Inner Monologue,huang2022innermonologueembodiedreasoning,22.07,可信-准确,指令理解,,,,,,,,,,
Integrating Disambiguation and User Preferences into Large Language Models for Robot Motion Planning,Abugurain et al.,abugurain2024integrating,24.04,可信-准确,指令理解,Mohammed Abugurain,KAUST,,,Navigation,LLM,,https://arxiv.org/abs/2404.14547,关键词：指令歧义、用户偏好,
Jailbreaking LLM-Controlled Robots,Robey et al.,robey2024jailbreakingllmcontrolledrobots,24.1,安全-抗攻击,指令理解,Alexander Robey,UPenn,,,EQA(2D),"Benchmark, Prompt",ICRA 2025,https://arxiv.org/abs/2410.13691,,
LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following,LACMA,yang2023lacmalanguagealigningcontrastivelearning,23.1,可信-可靠,指令理解,,,,,,,,,,
"LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions",Azeem et al.,azeem_llm-driven_2024,24.06,安全-价值对齐,指令理解,,,,,,,,,,
MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models,MM-SafetyBench,liu2024mmsafetybenchbenchmarksafetyevaluation,23.11,安全-抗攻击,指令理解,Xin Liu,"PJLAB, ECNU","Yunshi Lan, Chao Yang","PJLAB, ECNU",EQA(2D),"Benchmark, MLLM",ECCV 2024,https://arxiv.org/abs/2311.17600,,
NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place Exploration,NarraGuide,hu2025narraguide,25.08,可信-准确,指令理解,,,,,,,,,,
Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning,NoisyEQA,wu2024noisyeqabenchmarkingembodiedquestion,22.11,可信-准确,指令理解,Yunchao Zhang,UCSC,,,VLN,"指令微调, MLLM",ACL 2024,https://arxiv.org/abs/2211.14769,,
Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models,HELPER,sarch2023openendedinstructableembodiedagents,23.1,可信-准确,指令理解,,,,,,,,,,
POEX: Understanding and Mitigating Policy Executable Jailbreak Attacks against Embodied AI,POEX,lu2025poexunderstandingmitigatingpolicy,24.12,安全-抗攻击,指令理解,Xuancun Lu,ZJU,,,"Planning, EQA(2D)","MLLM, Benchmark, Prompt",,https://arxiv.org/abs/2412.16633,,
Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents,Yang et al.,yang2023plugsafetychipenforcing,23.09,安全-防滥用,指令理解,,,,,,,,,,
"Security Considerations in AI-Robotics: A Survey of Current Methods, Challenges, and Opportunities",Neupane et al.,neupane2024securityconsiderationsairoboticssurvey,23.1,安全-隐私可保护,指令理解,,,,,,,,,,
Semantic Skill Grounding for Embodied Instruction-Following in Cross-Domain Environments,SemGro,shin2024semanticskillgroundingembodied,24.08,可信-可靠,指令理解,,,,,,,,,,
ThinkBot: Embodied Instruction Following with Thought Chain Reasoning,ThinkBot,lu2023thinkbotembodiedinstructionfollowing,23.12,可信-准确,指令理解,,,,,,,,,,
Towards Robust Multimodal Large Language Models Against Jailbreak Attacks,SAFEMLLM,yin2025robustmultimodallargelanguage,25.02,安全-抗攻击,指令理解,Ziyi Yin,UPenn,,,EQA(2D),"adversarial attack, MLLM",,https://arxiv.org/abs/2502.00653,,
Verifiably Following Complex Robot Instructions with Foundation Models,LIMP,quartey2025verifiablyfollowingcomplexrobot,24.02,可信-可靠,指令理解,,,,,,,,,,
tagE: Enabling an Embodied Agent to Understand Human Instructions,tagE,sarkar2023tageenablingembodiedagent,23.1,可信-准确,指令理解,,,,,,,,,,
,,,,,指令理解,,,,,,,,,,
A Survey on Adversarial Robustness of LiDAR-based Machine Learning Perception in Autonomous Vehicles,Kim et al.,kim_survey_2024,24.11,"安全-抗攻击, 安全-防滥用",环境感知,Junae Kim,,,,survey,survey,,https://arxiv.org/pdf/2411.13778v1,环境感知,
Active SLAM With Dynamic Viewpoint Optimization for Robust Visual Navigation,Li et al.,li2025active,25.06,"可信-准确, 可信-可靠",环境感知,Peng Li,Chinese Academy of Sciences,Zhengxing Wu,University of Chinese Academy of Sciences,Navigation,SLAM,IEEE Transactions on Instrumentation and Measurement 2025,https://ieeexplore.ieee.org/abstract/document/11037221,,
Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation,Malone et al.,malone_adversarial_2025,25.01,"安全-抗攻击, 安全-防滥用",环境感知,Connor Malone,Queensland University of Technology,Michael Milford,Queensland University of Technology,Visual Place Recognition,"adversarial attack, Defence",IROS 2025,https://arxiv.org/pdf/2506.15988,环境感知,
An Enactive Approach to Value Alignment in Artificial Intelligence: A Matter of Relevance,Cannon et al.,cannon2021enactive,25.11,安全-价值对齐,环境感知,,,,,,,,,,
AuditMAI: Towards An Infrastructure for Continuous AI Auditing,AuditMAI,waltersdorfer2024auditmai,24.06,可信-可审计,环境感知,,,,,,,,,,
BadDepth: Backdoor Attacks Against Monocular Depth Estimation in the Physical World,BadDepth,guo_baddepth_2025,25.05,"安全-抗攻击, 安全-防滥用",环境感知,Ji Guo,USTC,Jiaming He,USTC,SLAM,adversarial attack,,https://arxiv.org/pdf/2505.16154,环境感知,
E2CL: exploration-based error correction learning for embodied agents,E2CL,wang2024e2cl,24.09,可信-可解释,环境感知,,,,,,,,,,
Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches,EAD,wu2024embodied,24.03,"安全-抗攻击, 安全-防滥用",环境感知,,,,,,,,,,
Embodied Laser Attack:Leveraging Scene Priors to Achieve Agent-based Robust Non-contact Attacks,ELA,sun2024embodied,24.07,"安全-抗攻击, 安全-防滥用",环境感知,,,,,,,,,,
Embodied Uncertainty-Aware Object Segmentation,UNCOS,fang2024embodied,24.1,可信-准确,环境感知,fang2024embodied,MIT,,,Segmentation,不确定度,IROS 2024,https://ieeexplore.ieee.org/abstract/document/10801562,,
Embodied active domain adaptation for semantic segmentation via informative path planning,"Zurbr{\""u}gg et al.",zurbrugg2022embodied,22.1,可信-准确,环境感知,René Zurbrügg,ETH Zurich,René Zurbrügg,ETH Zurich,Segmentation,Domain Adaptation,IEEE Robotics and Automation Letters 2022,https://ieeexplore.ieee.org/abstract/document/9816133,,
Embodied videoagent: Persistent memory from egocentric videos and embodied sensors enables dynamic scene understanding,Embodied VideoAgent,fan2024embodied,25.01,可信-可解释,环境感知,,,,,,,,,,
Embodied visual active learning for semantic segmentation,Nilsson et al.,nilsson2021embodied,21.12,可信-准确,环境感知,David Nilsson,"Lund University, Google",,,Segmentation,"Active Learning, Self-supervised",AAAI 2021,https://ojs.aaai.org/index.php/AAAI/article/view/16338,,
Embodiedgpt: Vision-language pre-training via embodied chain of thought,EmbodiedGPT,mu2023embodiedgpt,23.05,可信-准确,环境感知,Yao Mu,HKU,Ping Luo,HKU,"EQA(2D), Caption",LLM,NeurIPS 2023,https://proceedings.neurips.cc/paper_files/paper/2023/file/4ec43957eda1126ad4887995d05fae3b-Paper-Conference.pdf,,
Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai,EmbodiedScan,wang2024embodiedscan,24.03,可信-准确,环境感知,Tai Wang,Shanghai AI Laborator,Jiangmiao Pang,Shanghai AI Laborator,Grounding (3D),Benchmark,CVPR 2024,https://openaccess.thecvf.com/content/CVPR2024/html/Wang_EmbodiedScan_A_Holistic_Multi-Modal_3D_Perception_Suite_Towards_Embodied_AI_CVPR_2024_paper.html,,
Enhancing embodied object detection through language-image pre-training and implicit object memory,Chapman et al.,chapman2024enhancing,24.02,可信-准确,环境感知,Nicolas Harvey Chapman,,,,Object Detection,,arxiv 2024,https://arxiv.org/pdf/2402.03721,,
From Strangers to Assistants: Fast Desire Alignment for Embodied Agent-User Adaptation,Wang et al.,wang2025strangers,25.05,安全-价值对齐,环境感知,,,,,,,,,,
Good time to ask: A learning framework for asking for help in embodied visual navigation,GTA,zhang2023good,23.06,可信-可解释,环境感知,,,,,,,,,,
Improved Semantic Segmentation from Ultra-Low-Resolution RGB Images Applied to Privacy-Preserving Object-Goal Navigation,Huang et al.,huang2025improved,25.07,安全-隐私可保护,环境感知,,,,,,,,,,
Interactive task learning via embodied corrective feedback,Appelgren et al.,appelgren2020interactive,20.09,可信-可解释,环境感知,,,,,,,,,,
Interactron: Embodied adaptive object detection,INTERACTRON,kotar2022interactron,22.03,可信-准确,环境感知,Klemen Kotar,Allen Institute for AI,,,Object Detection,"Self-supervised, Active Learning",CVPR 2022,https://openaccess.thecvf.com/content/CVPR2022/papers/Kotar_Interactron_Embodied_Adaptive_Object_Detection_CVPR_2022_paper.pdf,,
Is the robot spying on me? a study on perceived privacy in telepresence scenarios in a care setting with mobile and humanoid robots,Nieto et al.,nieto2024robot,24.08,安全-隐私可保护,环境感知,,,,,,,,,,
Learn how to see: collaborative embodied learning for object detection and camera adjusting,STF,shen2024learn,24.03,可信-准确,环境感知,Lingdong Shen,University of Chinese Academy of Sciences,Chunlei Huo,University of Chinese Academy of Sciences,Object Detection,,AAAI 2024,https://ojs.aaai.org/index.php/AAAI/article/view/28281,,
Learning robust perceptive locomotion for quadrupedal robots in the wild,Miki et al.,miki2022learning,22.01,"可信-准确, 可信-可靠",环境感知,TAKAHIRO MIKI,ETH Zurich,,,Locomotion,强化学习,Science Robotics 2022,https://arxiv.org/abs/2201.08117,关键词：传感器外部感知；需要讨论是否属于范畴,
Learning to Walk by Steering: Perceptive Quadrupedal Locomotion in Dynamic Environments,PRELUDE,seo2023learning,22.09,"可信-准确, 可信-可靠",环境感知,Mingyo Seo,The University of Texas at Austin,,,Locomotion,强化学习,arxiv 2022,https://arxiv.org/abs/2209.09233,,
Legged locomotion in challenging terrains using egocentric vision,Agarwal et al.,agarwal2023legged,22.11,"可信-准确, 可信-可靠",环境感知,Ananye Agarwal,Carnegie Mellon University,Deepak Pathak,Carnegie Mellon University,Locomotion,强化学习,CoRL 2023,https://proceedings.mlr.press/v205/agarwal23a.html,,
Monitoring and Diagnosability of Perception Systems,Antonante et al.,antonante2021monitoring,20.11,可信-可审计,环境感知,,,,,,,,,,
Move to see better: Self-improving embodied object detection,Fang et al. ,fang2020move,20.12,可信-准确,环境感知,Zhaoyuan Fang,Carnegie Mellon University,,,Object Detection,Active Learning,bmvc2021,https://www.bmvc2021-virtualconference.com/assets/papers/0615.pdf,,
Obstacle-Aware Quadrupedal Locomotion With Resilient Multi-Modal Reinforcement Learning,Nahrendra et al.,nahrendra2024obstacle,24.09,"可信-准确, 可信-可靠",环境感知,I Made Aswin Nahrendra,KAIST,Hyun Myung,KAIST,Locomotion,强化学习,arxiv 2024,https://arxiv.org/abs/2409.19709,,
On the Sensory Commutativity of Action Sequences for Embodied Agents,SCP,caselles2020sensory,21.01,安全-价值对齐,环境感知,,,,,,,,,,
Openvla: An open-source vision-language-action model,OpenVLA,kim2024openvla,24.06,可信-准确,环境感知,Moo Jin Kim,Stanford University,,,VLA,VLA,arxiv 2024,https://arxiv.org/abs/2406.09246,,
Palm-e: An embodied multimodal language model,PALM-E,driess2023palm,23.07,可信-准确,环境感知,Danny Driess,"Google, TU Berlin",,,"EQA(2D), Manipulation, Caption",MLLM,ICML 2023,https://dl.acm.org/doi/abs/10.5555/3618408.3618748,,
Perception Matters: Enhancing Embodied AI with Uncertainty-Aware Semantic Segmentation,Prasanna et al.,prasanna2024perception,24.08,可信-准确,环境感知,Sai Prasanna,University of Freiburg,,,"Object Search, Segmentation",不确定度,arxiv 2024,https://arxiv.org/pdf/2408.02297,,
Privacy Risks of Robot Vision: A User Study on Image Modalities and Resolution,Huang et al.,huang2025privacy,25.05,安全-隐私可保护,环境感知,,,,,,,,,,
Privacy beyond Data: Assessment and Mitigation of Privacy Risks in Robotic Technology for Elderly Care,Grabler et al.,grabler2025privacy,24.11,安全-隐私可保护,环境感知,,,,,,,,,,
Privacy-preserving robot vision with anonymized faces by extreme low resolution,Kim et al.,kim2019privacy,19.11,安全-隐私可保护,环境感知,,,,,,,,,,
RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control,RT,pmlr-v229-zitkovich23a,23.07,可信-准确,环境感知,,Google DeepMind,,,VLA,VLA,CoRL 2023,https://proceedings.mlr.press/v229/zitkovich23a.html,,
Random Spoofing Attack against Scan Matching Algorithm SLAM (Long),Fukunaga et al.,noauthor_random_nodate,24.02,"安全-抗攻击, 安全-防滥用",环境感知,,,,,,,,,,
Real-time privacy preservation for robot visual perception,PCVS,choi2025real,25.05,安全-隐私可保护,环境感知,,,,,,,,,,
Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-to-End,Zhang et al.,zhang2024resilient,23.1,"可信-准确, 可信-可靠",环境感知,Jin Jin,ETH Zurich,,,Navigation,强化学习,ICRA 24,https://ieeexplore.ieee.org/abstract/document/10611254,关键词：感知失效,
Robot Manipulation Based on Embodied Visual Perception: A Survey,Wang et al.,wang2025robot,25.06,可信-准确,环境感知,Sicheng Wang,The Shenzhen Institute of Artificial Intelligence and Robotics for Societ,Tianwei Zhang,The Shenzhen Institute of Artificial Intelligence and Robotics for Society,survey,survey,CAAI Transactions on Intelligence Technology 2025,https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/cit2.70022,,
Robustnav: Towards benchmarking robustness in embodied navigation,RobustNAV,chattopadhyay2021robustnav,21.06,"可信-准确, 可信-可靠",环境感知,Prithvijit Chattopadhyay,Georgia Tech,,,Navigation,Benchmark,ICCV 2021,https://openaccess.thecvf.com/content/ICCV2021/html/Chattopadhyay_RobustNav_Towards_Benchmarking_Robustness_in_Embodied_Navigation_ICCV_2021_paper.html,,
Robustness of embodied point navigation agents,Frano Rajiˇc,rajivc2022robustness,23.02,"可信-准确, 可信-可靠",环境感知,Frano Rajiˇc,Swiss Federal Institute of Technology Lausanne,Frano Rajiˇc,Swiss Federal Institute of Technology Lausanne,Navigation,,ECCV 2022,,,
SLAMSpoof: Practical LiDAR Spoofing Attacks on Localization Systems Guided by Scan Matching Vulnerability Analysis,SLAMSpoof,noauthor_slamspoof_nodate,25.02,"安全-抗攻击, 安全-防滥用",环境感知,Rokuto Nagata,Amano Institute of Technology,Kentaro Yoshioka,Amano Institute of Technology,"Auto Drive, Navigation","Point-Wise SMVS, Frame-Wise SMVS",,https://arxiv.org/html/2502.13641v1,环境感知,
Safety Assessment for Autonomous Systems' Perception Capabilities,Molloy et al.,molloy2022safety,22.08,可信-可审计,环境感知,,,,,,,,,,
"Self-Explainable Affordance Learning with 
Embodied Caption",SEA,zhang2024self,24.04,可信-可解释,环境感知,Zhipeng Zhang,Northwestern Polytechnical University,,,Visual affordance learning,Benchmark,,https://arxiv.org/abs/2404.05603,,
SoK: Rethinking Sensor Spoofing Attacks against Robotic Vehicles from a Systematic View,SoK,xu2023sok,23.07,"安全-抗攻击, 安全-防滥用",环境感知,,,,,,,,,,
Towards Embodied Agent Intent Explanation in Human-Robot Collaboration: ACT Error Analysis and Solution Conceptualization,CRIE,ergogo2025towards,25.05,可信-可解释,环境感知,,,,,,,,,,
Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks,Xing et al.,xing2025robustsecureembodiedai,25.02,"安全-抗攻击, 安全-防滥用",环境感知,WENPENG XING,ZJU,MENG HAN,ZJU,survey,survey,,https://arxiv.org/pdf/2502.13175,物理交互,
Viewinfer3d: 3d visual grounding based on embodied viewpoint inference,ViewInfer3D,geng2024viewinfer3d,24.07,可信-准确,环境感知,Liang Geng,Beijing University of Posts and Telecommunications,Jianqin Yin,Beijing University of Posts and Telecommunications,Grounding (3D),LLM,IEEE Robotics and Automation Letters 2024,https://ieeexplore.ieee.org/abstract/document/10592798,,
Vima: General robot manipulation with multimodal prompts,VIMA,jiang2022vima,22.1,可信-准确,环境感知,Yunfan Jiang,Stanford University,Yuke Zhu,NVIDIA,Manipulation,Benchmark,arxiv 2022,https://arxiv.org/abs/2210.03094,,
What do navigation agents learn about their environment?,iSEE,dwivedi2022navigation,22.06,可信-可解释,环境感知,Kshitij Dwivedi,Goethe University Frankfurt,,,VLN,Visualization,CVPR'22,https://openaccess.thecvf.com/content/CVPR2022/html/Dwivedi_What_Do_Navigation_Agents_Learn_About_Their_Environment_CVPR_2022_paper.html,,
Ask4Help: Learning to Leverage an Expert for Embodied Tasks,Ask4Help,singh2022ask4help,22.11,可信-可靠,决策规划,Kunal Pratap Singh ,Allen Institute for AI,,,"Navigation, Room Rearrangement",强化学习,NeurIPS 22,https://arxiv.org/abs/2211.09960,关键词：交互,
BadRobot: Jailbreaking Embodied LLMs in the Physical World,BadRobot,zhang2025badrobotjailbreakingembodiedllms,24.07,安全-抗攻击,"决策规划, 指令理解",Hangtao Zhang,HUST,,,"Planning, EQA(2D)",Attack Paradigm,ICLR 2025,https://arxiv.org/abs/2407.20242,,
BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization,BadVLA,zhou2025badvla,25.05,安全-抗攻击,决策规划,Xueyang Zhou,HUST,Lichao Sun,Lehigh University,VLA,adversarial attack,,https://arxiv.org/pdf/2505.16640,行为规划,
Characterizing Physical Adversarial Attacks on Robot Motion Planners,Wu et al.,wu2024characterizing,24.01,安全-抗攻击,决策规划,Wenxi Wu,KCL,Wenxi Wu,KCL,Planning,"物理干预, adversarial attack",ICRA 2024,https://kclpure.kcl.ac.uk/ws/portalfiles/portal/248844190/icra2024_motion_planning_attacks.pdf,行为规划,
"Disability 4.0: bioethical considerations on the use of embodied artificial intelligence

",Disability 4.0,de2024disability,24.08,安全-价值对齐,决策规划,Francesco De Micco,University Campus Bio-Medico of Rome,Roberto Scendoni,"University of Macerata,  Macerata","survey, Manipulation",survey,,https://www.frontiersin.org/journals/medicine/articles/10.3389/fmed.2024.1437280/full,,
" DynaMem: Online Dynamic
Spatio-Semantic Memory for Open World Mobile
Manipulation",DynaMem,liuDynaMemOnlineDynamic2025,25.05,可信-准确,决策规划,Peiqi Liu,NYU,,,,,,https://arxiv.org/pdf/2411.04999,,
EHAZOP: A Proof of Concept Ethical Hazard Analysis of an Assistive Robot,EHAZOP,menon2024ehazop,24.06,可信-可控,决策规划,,,,,,,,https://arxiv.org/pdf/2406.09239,,
Ella: Embodied Social Agents with Lifelong Memory,Ella,zhangEllaEmbodiedSocial2025,25.06,可信-准确,决策规划,Hongxin Zhang,University of Massachusetts Amherst,,,,,,https://arxiv.org/pdf/2506.24019,,
Embodied Escaping: End-to-End Reinforcement Learning for Robot Navigation in Narrow Environment,Embodied Escaping,zheng2025embodied,25.03,可信-可靠,决策规划,Han Zheng,SJTU,,,Navigation,强化学习,,https://arxiv.org/abs/2503.03208,关键词：狭窄环境,
"Embodied-RAG: General Non-
parametric Embodied Memory for Retrieval and
Generation",Embodied-RAG,xieEmbodiedRAGGeneralNonparametric2025,25.01,可信-准确,决策规划,Quanting Xie,CMU,,,,,,https://arxiv.org/pdf/2409.18313,,
Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models,Liu et al.,liu2024exploring,24.05,安全-抗攻击,决策规划,Shuyuan Liu,ECNU,Zhaoxia Yin,ECNU,Planning,"adversarial attack, MLLM",ACM MM 2024,https://arxiv.org/abs/2405.19802,,
From Screens to Scenes: A Survey of Embodied AI in Healthcare,Naik ,naik2022legal,25.03,可信-可审计,决策规划,Yihao Liu,Central South University,", Jintai Chen",HKUST(Guangzhou),survey,survey,,https://arxiv.org/abs/2501.07468,,
Generating Explanations for Embodied Action Decision from Visual Observation,Wang et al.,wang2023generating,23.1,可信-可解释,决策规划,Xiaohan Wang,XJTU,,,"Planning, VLN",Benchmark,MM'23,https://dl.acm.org/doi/10.1145/3581783.3612351,,
Generative Agents: Interactive Simulacra of Human Behavior,generative agents,parkGenerativeAgentsInteractive2023,23.08,可信-准确,决策规划,Joon Sung Park,Stanford University,,,,,,https://arxiv.org/pdf/2304.03442,,
"Habitat-web: Learning embod-
ied object-search strategies from human demonstra-
tions at scale",Habitat-web,ramrakhya2022habitat,22.04,可信-可靠,决策规划,,,,,,,,https://arxiv.org/abs/2204.03514,,
"HomeRobot: Open-Vocabulary Mobile
Manipulation",HomeRobot: ,yenamandraHomeRobotOpenVocabularyMobile2024,23.06,可信-准确,决策规划,Sriram Yenamandra,Georgia Tech,,,,,,https://arxiv.org/pdf/2306.11565,,
Humanizing AI in medical training: ethical framework for responsible design,Tahri Sqalli et al.,tahri2023humanizing,23.05,安全-价值对齐,决策规划,Mohammed Tahri Sqalli,GUQ,Mohammed Tahri Sqalli,GUQ,survey,理论框架,,https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2023.1189914/full,,
"INTRODUCING THE Robot Security Framework (RSF), A STANDARDIZED METHODOLOGY TO PERFORM SECURITY ASSESSMENTS IN ROBOTICS",RSF,adebayo2023introducing,23.12,可信-可审计,决策规划,Abiodun Sunday Adebayo,University of Staffordshire,Abiodun Sunday Adebayo,University of Staffordshire,Manipulation,"混合模型, 理论框架",Journal of Frontiers in Multidisciplinary Research,https://www.multidisciplinaryfrontiers.com/uploads/archives/20250312183510_FMR-2025-1-004.1.pdf,,
LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning,LLaPa,sun2025llapa,25.07,可信-可靠,决策规划,Shibo Sun,HIT,,,,,,https://arxiv.org/pdf/2507.08496,,
Manipulating Neural Path Planners via Slight Perturbations,Xiong et al.,xiong2024manipulating,24.03,安全-隐私可保护,决策规划,Zikang Xiong,Purdue University," Suresh Jagannathan",Purdue University,Navigation,Backdoor Attack,,https://arxiv.org/pdf/2403.18256,行为规划,
Modeling Dynamic Environments with Scene Graph Memory,"Scene Graph
Memory",kurenkovModelingDynamicEnvironments2023,23.06,可信-准确,决策规划,Andrey Kurenkov,Stanford University,,,,,,https://arxiv.org/pdf/2305.17537,,
Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration,Borazjani et al.,borazjani2025multi,25.05,安全-隐私可保护,决策规划,Kasra Borazjani,,,,survey,联邦学习,,https://arxiv.org/abs/2505.11191,,
REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS,REACT,yao2023react,23.05,可信-准确,决策规划,Shunyu Yao,Princeton University,,,,,ICLR2023,https://arxiv.org/pdf/2210.03629,,
"ReMEmbR: Building
and Reasoning Over Long-Horizon Spatio-Temporal
Memory for Robot Navigation",ReMEmbR,anwarReMEmbRBuildingReasoning2024,24.09,可信-准确,决策规划,Abrar Anwar,"NVIDIA, University of Southern California",,,,,,https://arxiv.org/pdf/2409.13682,,
Reinforced Reasoning for Embodied Planning,Wu et al.,wu2025reinforced,25.05,可信-准确,决策规划,Di Wu,SJTU,,,VLN,"强化学习, 指令微调, MLLM",,https://arxiv.org/abs/2505.22050,关键词：长程规划,
Robo-Troj: Attacking LLM-based Task Planners,Robo-Troj,nahian2025robo,25.04,可信-准确,决策规划,Mohaiminul Al Nahian; Zainab Altaweel,SUNY Binghamton,Adnan Siraj Rakin,SUNY Binghamton,"Manipulation, Planning","指令微调, LLM",,https://arxiv.org/pdf/2504.17070,行为规划,
SEMI-PARAMETRIC TOPOLOGICAL MEMORY FOR NAVIGATION,SPTM,savinovSemiparametricTopologicalMemory2018,18.05,可信-准确,决策规划,Nikolay Savinov,ETH Zurich,,,,,,https://arxiv.org/pdf/1803.00653,,
SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents,SafeAgentBench,yin2024safeagentbench,24.12,可信-准确,决策规划,Sheng Yin,SJTU,Siheng Chen,PJLAB,Planning,"LLM, Prompt, AI2-THOR, Benchmark",,https://arxiv.org/abs/2412.13178,,
Safety Aware Task Planning via Large Language Models in Robotics,SAFER,khan2025safety,25.03,可信-可控,决策规划,Azal Ahmad Khan,University of Minnesota,,,"Planning, Manipulation","LLM, Prompt",,https://arxiv.org/pdf/2503.15707,,
Safety Control of Service Robots with LLMs and Embodied Knowledge Graphs,Qi et al. ,qi2024safety,24.05,可信-准确,决策规划,Yong Qi,陕西科技大学,,,"Manipulation, Planning","LLM, Prompt, Knowledge Graph",,https://arxiv.org/pdf/2405.17846,,
Safety assurances for human-robot interaction via confidence-aware game-theoretic human models,Tian et al.,tian2022safety,21.1,可信-可控,决策规划,,,,,,,,,,
"SnapMem: Snapshot-based 3D Scene Memory for
Embodied Exploration and Reasoning",SnapMem,yangSnapMemSnapshotbased3D2024,24.11,可信-准确,决策规划,Yuncong Yang,UMass Amherst,,,,,,https://arxiv.org/html/2411.17735v1,,
"Thinking in Space:
How Multimodal Large Language Models See, Re-
member, and Recall Spaces",Thinking in Space,yangThinkingSpaceHow2025,24.12,可信-准确,决策规划,Jihan Yang,NYU,,,,,,https://arxiv.org/pdf/2412.14171,,
Trust-aware motion planning for human-robot collaboration under distribution temporal logic specifications,Yu et al.,yu2024trust,23.1,可信-可控,决策规划,,,,,,,,,,
Uncertainty in Action: Confidence Elicitation in Embodied Agents,Yu et al. ,yu2025uncertainty,25.03,可信-可靠,决策规划,Tianjiao Yu,UIUC,,,"Planning, VLN",不确定度,,https://arxiv.org/abs/2503.10628,关键词：置信度,
VLM-Social-Nav: Socially Aware Robot Navigation through Scoring using Vision-Language Models,VLM-Social-Nav,song2024vlm,24.11,可信-可控,决策规划,,,,,,,,,,
Who’s in Charge Here? A Survey on Trustworthy AI in Variable Autonomy Robotic Systems,Methnani et al.,methnani2024s,24.04,"可信-可控, 安全-价值对齐","决策规划, 指令理解",Leila Methnani,UmU,,,"survey, Auto Drive",理论框架,,https://dl.acm.org/doi/pdf/10.1145/3645090,,
6-DOF Grasping for Target-driven Object Manipulation in Clutter," Collision-
Net",murali20206,19.12,可信-可靠,物理交互,Adithyavairavan Murali,NVIDIA,,,,,,https://arxiv.org/abs/1912.03628,,
A Review of Future and Ethical Perspectives of Robotics and AI,Torrensen（2018）,torresen2017review,18.1,"安全-隐私可保护, 安全-价值对齐",物理交互,Jim Torresen,University of Oslo,Jim Torresen,University of Oslo,"survey, Manipulation","survey, 理论框架",,https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2017.00075/full,,
A Secure Robot Learning Framework for Cyber Attack Scheduling and Countermeasure,Wu et al.,wu_secure_2023,23.06,安全-防滥用,物理交互,Chengwei Wu,HIT,Ligang Wu,HIT,,,TRO 2023,https://ieeexplore.ieee.org/document/10144090,物理交互的攻防问题,
AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective,AdvGrasp,wang_advgrasp_2025,25.07,安全-抗攻击,物理交互,Xiaofei Wang,USTC,Yunbo Zhao；Keke Tang,"USTC, Guangzhou University",Grasp,"adversarial attack, 模拟退火算法",,https://arxiv.org/pdf/2507.09857,物理交互,
Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes,Contact-GraspNet,sundermeyer2021contact,21.05,可信-可靠,物理交互,Martin Sundermeyer,NVIDIA,,,,,,https://arxiv.org/abs/2103.14127,,
"Controllability, Observability, Realizability, and Stability of Dynamic Linear Systems",Davis et al.,davis_controllability_2009,9.01,可信-可控,物理交互,John M. Davis,,,,,,,,,
Denoising diffusion probabilistic models,DDPM,ho2020denoising,20.06,可信-可靠,物理交互,Jonathan Ho,UCB,,,,,,https://arxiv.org/pdf/2006.11239,,
Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects,Dex-NeRF,ichnowski2022dex,21.1,可信-可靠,物理交互,Jeffrey Ichnowski,UCB,,,,,,https://arxiv.org/pdf/2110.14217,,
Diffusion Policy: Visuomotor Policy Learning via Action Diffusion,Diffusion Policy,chi2023diffusion,23.03,可信-可靠,物理交互,Cheng Chi,Columbia University,,,,,,https://arxiv.org/abs/2303.04137,,
Disability 4.0: bioethical considerations on the use of embodied artificial intelligence,Disability 4.0,demicco2024disability,24.08,安全-隐私可保护,物理交互,,,,,,,,,,
Form2Fit: Learning Shape Priors for Generalizable Assembly from Disassembly,Form2Fit,zakka2020form2fit,19.1,可信-可靠,物理交互,Kevin Zakka,Stanford University,,,,,,https://arxiv.org/pdf/1910.13675,,
From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot Control,LCB,shentu2024llmsactionslatentcodes,24.05,可信-可靠,物理交互,,,,,,,,,,
Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation,HDP,ma2024hierarchical,24.03,可信-可靠,物理交互,Xiao Ma,Dyson Robot Learning Lab,,,,,,https://arxiv.org/pdf/2403.03890,,
"Humanoid Robots in Tourism and Hospitality—Exploring Managerial, Ethical, and Societal Challenges",Skubis et al.,skubis2024humanoid,24.12,"安全-隐私可保护, 安全-价值对齐",物理交互,Ida Skubis,Silesian University of Technology,Agata Mesjasz-Lech,Czestochowa University of Technology,survey,survey,,https://www.mdpi.com/2076-3417/14/24/11823,,
Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning,ViLa,hu2023look,23.11,可信-可靠,物理交互,,,,,,,,,,
On the general theory of control systems,Kalman(1960),kalman_general_1960,1959.12,可信-可控,物理交互,,,,,,,,,,
Optimal Actuator Attacks on Autonomous Vehicles Using Reinforcement Learning ,Wang et al.,wang_optimal_2025,25.02,安全-防滥用,物理交互,Pengyu Wang,HKUST,Ling Shi,HKUST,Auto Drive,强化学习,,https://arxiv.org/pdf/2502.07839,"行为规划, 调整控制量，让注入的控制量能够躲过检测器的检测，又能够成功改变目标的轨迹",
Partmanip: Learning cross-category generalizable part manipulation policy from point cloud observations,Partmanip,geng2023partmanip,23.03,可信-可靠,物理交互,Haoran Geng,CFCS,,,,,,https://arxiv.org/pdf/2303.16958,,
Planning with Diffusion for Flexible Behavior Synthesis,Diffuser,janner2022planning,22.05,可信-可靠,物理交互,,,,,,,,,,
RIC: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction,RIC,kasahara2023ric,23.07,可信-可靠,物理交互,Isaac Kasahara,Samsung AI Center,,,,,,https://arxiv.org/pdf/2307.11932,,
Robust Humanoid Locomotion Using Trajectory Optimization and Sample-Efficient Learning,Yeganegi et al.,yeganegi_robust_2019,19.07,安全-抗攻击,物理交互,,,,,,,,,,
Robust Push Recovery on Bipedal Robots: Leveraging Multi-Domain Hybrid Systems with Reduced-Order Model Predictive Control,Dai et al.,dai_robust_2025,25.04,安全-抗攻击,物理交互,Min Dai,,,,,,,,,
Se (3)-diffusionfields: Learning smooth cost functions for joint grasp and motion optimization through diffusion,Se (3)-diffusionfields,urain2023se,23.07,可信-可靠,物理交互,Julen Urain;Niklas Funk,Technische Universitat Darmstadt (Germany),,,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161569,,
SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution,SkillDiffuser,liang2023skilldiffuser,23.12,可信-可靠,物理交互,,,,,,,,,,
TransCG: A Large-Scale Real-World Dataset for Transparent Object Depth Completion and a Grasping Baseline,Fang et al.,fang2022transcg,22.02,可信-可靠,物理交互,Hongjie Fang,SJTU,,,,,,https://arxiv.org/abs/2202.08471,,
Vision-Language-Action Model and Diffusion Policy Switching Enables Dexterous Control of an Anthropomorphic Hand,,pan2024visionlanguageactionmodeldiffusionpolicy,24.1,可信-可靠,物理交互,Cheng Pan,Swiss Federal Institute of Technology Lausanne,,,,,,,,
A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents,,,25.04,,,Yuting Huang,USTC,Mingxiao Ma; Yanyong Zhang,USTC,Planning,"指令微调, LLM, VirtualHome, Benchmark",,https://arxiv.org/abs/2504.14650,,
A Survey of Embodied Learning for Object-Centric Robotic Manipulation ,,,,,,,,,,survey,survey,,https://arxiv.org/pdf/2408.11537,,
A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI,,,,,,,,,,survey,survey,,https://arxiv.org/pdf/2505.01458,,
A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety,,,,,,,,,,,,,,,
A Survey on Bias and Fairness in Machine Learning,,,,,,,,,,,,,https://arxiv.org/abs/1908.09635,,
Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents,,,25.05,,,Zhejian Yang,Jiling University,,,VLA,MLLM,,https://arxiv.org/abs/2505.23450,,
AgiBot World Colosseo: Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems,,,25.03,,," Team AgiBot-World","HKU,  OpenDriveLab"," Team AgiBot-World","HKU,  OpenDriveLab","Manipulation, VLA","MLLM, Diffusion, 预训练, 指令微调",,https://arxiv.org/abs/2503.06669,,
"Building Trustworthy AI: Transparent AI 
Systems via Language Models, Ontologies, and 
Logical Reasoning (TranspNet)",,,24.12,,,Fadi Al Machot,Norwegian University of Life Science,,,LLM Reasoning,LLM,,https://arxiv.org/html/2411.08469v1,,
CAN WE TRUST EMBODIED AGENTS? EXPLORING BACKDOOR ATTACKS AGAINST EMBODIED LLM-BASED DECISION-MAKING SYSTEMS,,,24.05,,,Ruochen Jiao; Shaoyuan Xie,"Northwestern University , University of California",Ruochen Jiao; Shaoyuan Xie,"Northwestern University, University of California",VLA,"adversarial attack, VLA",ICLR 2025,https://arxiv.org/pdf/2405.20774,"决策规划,环境感知",
CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning ,,,25.06,,,Kailing Li,ECNU,,,EQA(2D),MLLM,,https://arxiv.org/pdf/2506.17629,,
Counterfactual collaborative reasoning,Ji et al.,ji2023counterfactual,23.06,,,Jianchao Ji,Rutgers University,,,,,,https://arxiv.org/pdf/2307.00165,,
Demystifying Embodied AI ,,,25.01,,,Sunil Sable,Vishwakarma Institute of Technology,S. Sable ,Vishwakarma Institute of Technology,survey,survey,,https://link.springer.com/chapter/10.1007/978-3-031-68256-8_2,,
DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following,DialFRED,gao2022dialfred,22.07,可信-准确,,Xiaofeng Gao,UCLA,,,EQA(2D),"Benchmark, 强化学习","IEEE Robotics and Automation Letters, 2022",https://arxiv.org/abs/2202.13330,关键词：指令对话,
" Diffusion Models for Robotic Manipulation: A Survey",,,,,,,,,,survey,survey,,https://arxiv.org/pdf/2504.08438,,
Earbench: Towards evaluating physical risk awareness for task planning of foundation model-based embodied ai agents,,,24.08,,,Zihao Zhu,CUHK-sz,Baoyuan Wu,CUHK-sz,Planning,"Benchmark, Prompt, LLM",,https://arxiv.org/abs/2408.04449,可能是最早的具身大脑的安全bench,
"Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity",,,,,,,,,,,,IROS,https://ieeexplore.ieee.org/abstract/document/10802181,,
"Embodied Agent Interface: Benchmarking LLMs for 
Embodied Decision Making",,,24.1,,,"Manling Li,Shiyu Zhao,Qineng Wang,Kangrui Wang,Yu Zhou",Stanford University,,,Planning,"LLM, Benchmark",,https://arxiv.org/abs/2410.07166,,
Embodied artificial intelligence in ophthalmology,,,25.06,,,Yao Qiu,The Hong Kong Polytechnic University,Mingguang He,The Hong Kong Polytechnic University,survey,survey,Nature子刊npj | digital medicine,https://www.nature.com/articles/s41746-025-01754-4.pdf,,
"Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks",,,25.03,,,Wenqi Zhang,ZJU,,,"Planning, VLN","LLM, 迭代式交互",,https://arxiv.org/abs/2503.21696,,
Embracing the Future: Navigating the Challenges and Solutions in Embodied Artificial Intelligence ,,,25.01,,,Wasim Khan,Koneru Lakshmaiah Education Foundation,W. Khan,Koneru Lakshmaiah Education Foundation,survey,"survey, 理论框架",,https://link.springer.com/chapter/10.1007/978-3-031-68256-8_13,,
Empowering Autonomous Driving with Large Language Models: A Safety Perspective,,,23.12,,,,,,,Auto Drive,,,,,
Ethical and regulatory challenges of AI technologies in healthcare: A narrative review,,,24.02,,,Ciro Mennella,ICAR,Umberto Maniscalco,ICAR,survey,survey,,https://www.cell.com/heliyon/fulltext/S2405-8440(24)02328-4,,
Everyday Object Meets Vision-and-Language Navigation Agent via Backdoor,,,24.11,,,Keji He,SDU,Liang Wang,自动化所,VLN,"指令微调, MLLM",NeurIPS 2024,https://openreview.net/forum?id=rXGxbDJadh&referrer=%5Bthe%20profile%20of%20Keji%20He%5D(%2Fprofile%3Fid%3D~Keji_He1),,
Explainable Embodied Agents Through Social Cues: A Review,,,21.07,,,SEBASTIAN WALLKÖTTER,Uppsala University and INESC-ID-Instituto Superior Técnico,,,survey,survey,ACM Transactions on Human-Robot Interaction,https://dl.acm.org/doi/abs/10.1145/3457188,,
Exploring Adversarial Obstacle Attacks in Search-based Path Planning for Autonomous Mobile Robots,,,25.04,,,Adrian Szvoren;,UCL,Dimitrios Kanoulas; Nilufer Tuptuk,UCL,Navigation,brute-force algorithm,,https://arxiv.org/pdf/2504.06154,行为规划,
Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics,,,24.11,,,Taowen Wang,Rochester Institute of Technology,Ruixiang Tang,Rutgers University,VLA,"VLA, adversarial attack",,https://arxiv.org/pdf/2411.13587,patch攻击,
FP3: A 3D Foundation Policy for Robotic Manipulation,,,25.03,,," Rujia Yang, Geng Chen","THU, PJLAB, UC San Diego","Chuan Wen, Yang Gao","THU, PJLAB, Shanghai Qi Zhi Institute",Manipulation,"Diffusion, Transformer, 指令微调, 预训练",,https://arxiv.org/pdf/2503.08950,,
Gripper Keypose and Object Pointflow as Interfaces for Bimanual Robotic Manipulation,,,25.04,,,Yuyin Yang; Zetao Cai,"FDU, ZJU, PJLAB",Jiangmiao Pang,PJLAB,"Manipulation, VLA","Diffusion, 预训练",,https://arxiv.org/abs/2504.17784,,
HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation,,,25.05,,,"Yi Li, Yuquan Deng, Jesse Zhang","NVIDIA, University of Washington, University of Southern California","Anqi Li, Abhishek Gupta, Ankit Goyal","NVIDIA, University of Washington",Manipulation,"MLLM, 模仿学习, 离线学习, 指令微调, 预训练, 规划算法",,https://arxiv.org/pdf/2502.05485,,
HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in Embodied Agents,,,25.03,,,Tristan Tomilin,Eindhoven University of Technology,,,"VLN, Planning",Benchmark,ICLR2025,https://arxiv.org/abs/2503.08241,,
HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments,,,,,,,,,,,,ICLR2024,,,
IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks,IS-Bench,lu2025bench,25.06,可信-准确,,Xiaoya Lu,PJLAB,Jing Shao; Dongrui Liu,PJLAB,Planning,"LLM, COT, Benchmark, OmniGibson模拟器",,https://arxiv.org/abs/2506.16402,,
IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios,,,25.05,,,Yifan Li,Michigan State University,Kong Yu,Michigan State University,EQA(2D),"Benchmark, Isaac Sim模拟器",,https://arxiv.org/abs/2505.20640,,
"KARMA: Augmenting Embodied AI Agents
with Long-and-short Term Memory Systems",KARMA,wangKARMAAugmentingEmbodied2025,24.09,,,Zixuan Wang,CASIA,,,,,,https://arxiv.org/pdf/2409.14908,,
Learning Safe Multi-Agent Control with Decentralized Neural Barrier Certificates,,,,,,,,,,,,,,,
"Learning Visually Grounded Domain Ontologies 
via Embodied Conversation and Explanation",Park et al.,park2025learning,25.04,,,Jonghyuk Park,University of Edinburgh,,,视觉理解,Teacher Feedback,AAAI'25,https://ojs.aaai.org/index.php/AAAI/article/view/33573,,
Learning to Explore using Active Neural SLAM,,chaplotLearningExploreUsing2020,20.04,,,Devendra Singh Chaplot,Carnegie Mellon University,,,,,,https://arxiv.org/pdf/2004.05155,,
Legal and Ethical Consideration in Artificial Intelligence in Healthcare: Who Takes Responsibility?,,,22.03,,,Nithesh Naik,Manipal Academy of Higher Education,,,survey,survey,,https://www.frontiersin.org/journals/surgery/articles/10.3389/fsurg.2022.862322/full?gclid=Cj0KCQi,,
ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation ,,,24.06,,,Xiaoqi Li,Peking University,,,VLA,MLLM,CVPR'24,https://openaccess.thecvf.com/content/CVPR2024/papers/Li_ManipLLM_Embodied_Multimodal_Large_Language_Model_for_Object-Centric_Robotic_Manipulation_CVPR_2024_paper.pdf,,
Memory for Robot Navigation,,,,,,,,,,,,,,,
MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation,,,25.03,,,Zhenyu Wu,Beijing University of Posts and Telecommunications," Haibin Yan",Beijing University of Posts and Telecommunications,Manipulation,"MLLM, 预训练, 指令微调, 规划算法",,https://arxiv.org/pdf/2503.13446,,
Multi-agent Embodied AI: Advances and Future Directions,,,25.05,,,Zhaohan Feng,Beijing Institute of Technology,,,survey,survey,,https://arxiv.org/pdf/2505.05108?,,
NoisyEQA: Benchmarking Embodied Question Answering Against Noisy Queries,,,24.12,,,Tao Wu,NTU,,,EQA(2D),Benchmark,,https://arxiv.org/abs/2412.10726,关键词：感知噪声,
Position: a call for embodied AI,,,,,,,,,,,,,https://openreview.net/forum?id=e5admkWKgV,,
Provably Safe Deep Reinforcement Learning for Robotic Manipulation in Human Environments,,,,,,,,,,,,ICRA2022,,,
RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation,,,25.03,,," Songming Liu; Lingxuan Wu",THU,Hang Su; Jun Zhu,THU,"Manipulation, VLA","Diffusion, 预训练, MLLM, LLM",,https://arxiv.org/abs/2410.07864,,
Random Spoofing Attack against LiDAR-Based Scan Matching SLAM,,,24.02,,,Masashi Fukunaga,Mitsubishi Electric Corporation,Takeshi Sugawara,The University of Electro-Communications,SLAM,对抗扰动,VehicleSec 2024,https://www.ndss-symposium.org/ndss-paper/auto-draft-476/,环境感知,
RiskAwareBench: Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based Embodied Agents,,,24.08,,,,,,,,,,https://arxiv.org/pdf/2408.04449v1,EARBench的初版,
RoboBERT: An End-to-end Multimodal Robotic Manipulation Model,,,25.05,,," Sicheng Wang, Sheng Liu, Weiheng Wang","Casbot Robotic Corporation, Karlsruhe Institute of Technology",Bin Fang,Beijing Universuty of Post and Telecommunicate,Manipulation,"MLLM, Diffusion, ViT, 离线学习, 模仿学习, 指令微调, 预训练",,https://arxiv.org/pdf/2502.07837,,
RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation,,,25.03,,,Xiaojian Li,,Hangjie Mo,,Manipulation,"MLLM, VLM, 预训练, 指令微调",,https://arxiv.org/pdf/2503.19510,,
RoboGround: Robotic Manipulation with Grounded Vision-Language Priors,,,25.04,,,Haifeng Huang; Xinyi Chen,"ZJU, PJLAB",Yilun Chen; ZhouZhao,"PJLAB, ZJU","Manipulation, VLA","LLM, MLLM, 预训练, 指令微调",,https://arxiv.org/abs/2504.21530,,
Safe Bayesian Optimization for the Control of High-Dimensional Embodied Systems,,,24.12,,,Yunyue Wei,THU,Yanan Sui,THU,VA,贝叶斯优化,CoRL2024,https://arxiv.org/pdf/2412.20350,,
Safe multi-agent reinforcement learning for multi-robot control,,,,,,,,,,,,,,,
"Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving",,,,,,,,,,,,,,,
SafeBench: A Benchmarking Platform for Safety Evaluation of Autonomous Vehicles,,,22.06,,,Chejian Xu,UIUC,,,Auto Drive,"Benchmark, ROS",NIPS2022 Track on Datasets and Benchmarks.,https://arxiv.org/pdf/2206.09682,,
SafeEmbodAI: a Safety Framework for Mobile Robots in Embodied AI Systems,,,,,,,,,,,,,https://arxiv.org/pdf/2409.01630,,
Securing Embodied AI: Addressing Cybersecurity Challenges in Physical Systems,,,,,,,,,,,,,https://link.springer.com/chapter/10.1007/978-3-031-68256-8_21,,
Securing the Future: Exploring Privacy Risks and Security Questions in Robotic Systems,,,,,,,,,,,,,,,
"Security Considerations in AI-Robotics:
A Survey of Current Methods,
Challenges, and Opportunities",,,24.01,,,SUBASH NEUPANE,Mississippi State University,Subash Neupane,Mississippi State University,"survey, Manipulation","survey, MLLM",,https://arxiv.org/pdf/2310.08565,,
Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing,,,25.01,,,Muhamamd Haris Khan; Selamawit Asfaw,Skoltech,,,"Manipulation, VLA","MLLM, LLM",,https://arxiv.org/abs/2501.06919,,
Situated Instruction Following,SIF,yenamandraHomeRobotOpenVocabularyMobile2024,24.07,,,So Yeon Min,CMU,,,,,,https://arxiv.org/pdf/2407.12061,,
Strategic Planning of Stealthy Backdoor Attacks in Markov Decision Processes,,,25.04,,,Xinyi Wei,UFL,Jie Fu,UFL,Manipulation,强化学习,,https://arxiv.org/pdf/2504.13276,行为规划,
"Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety of LLMs for Embodied Decision Making",,,25.05,,,Yejin Son,Yonsei University,Youngjae Yu,Yonsei University,Planning,"LLM, Benchmark",,https://arxiv.org/abs/2505.19933,,
Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours,,,15.09,,,,,,,Manipulation,,,,,
TLA: Tactile-Language-Action Model for Contact-Rich Manipulation,,,25.03,,," Peng Hao, Chaofan Zhang","Samsung Research China, Institute of Automation,  Chinese Academy of Sciences",,,Manipulation,"MLLM, 模仿学习, 离线学习, 指令微调, 预训练",,https://arxiv.org/pdf/2503.08548,,
"Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation",,,25.06,,," Yuanqi Yao, Yan Ding, Siao Liu, Bin Zhao, Haoming Song","PJLAB, FDU, SJTU, INSAIT,  Sofia University"," Dong Wang",PJLAB,Manipulation,"Diffusion, 离线学习",,https://arxiv.org/pdf/2504.00420,,
Towards Explainable Embodied AI,,,21.08,,,Vidhi Jain ,CMU,,,Planning,Feature Atrribution,硕士论文,https://www.ri.cmu.edu/app/uploads/2021/08/MSRoboticsThesis-2.pdf,,
Towards Explainable and Trustworthy Collaborative Robots through Embodied Question Answering,,,22.05,,,Lars Kunze,University of Oxford,,,EQA(2D),Knowledge Graph,,https://ora.ox.ac.uk/objects/uuid:4d8079cb-7633-417d-ba9b-d580aaebff64,,
"Unintended Consequences of Biased Robotic and Artificial Intelligence Systems [Ethical, Legal, and Societal Issues]",,,19.09,,,Ludovic Righetti,NYU,,,survey,理论框架,,https://ieeexplore.ieee.org/document/8825881,,
VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation,,,25.02,,,Siyu Xu,USYD," Chang Xu",USYD,Manipulation,"MLLM, 模仿学习, 离线学习, 指令微调",,https://arxiv.org/pdf/2502.02175,,
VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning,,,25.05,,,Guanxing Lu,"Tsinghua Shenzhen International Graduate School,  Tsinghua University",Yansong Tang,"Tsinghua Shenzhen International Graduate School,  Tsinghua University",Manipulation,"MLLM, 强化学习, 模仿学习, 在线学习, 离线学习, 预训练, 指令微调",,https://arxiv.org/pdf/2505.18719,,
VOYAGER: An Open-Ended Embodied Agent with Large Language Models ,,,23.05,,,Guanzhi Wang,NVIDIA,,,Embodied Task,LLM,,https://arxiv.org/pdf/2305.16291,,
World Models: The Safety Perspective,,,,,,,,,,,,,https://ieeexplore.ieee.org/abstract/document/10771431,世界模型安全（用于自动驾驶）,
and Reasoning Over Long-Horizon Spatio-Temporal,,,,,,,,,,,,,,,
iManip: Skill-Incremental Learning for Robotic Manipulation,,,25.03,,," Zexin Zheng",Sun Yat-sen University,,,Manipulation,"ViT, Transformer, 模仿学习, 离线学习, 增量学习",,https://arxiv.org/pdf/2503.07087,,
